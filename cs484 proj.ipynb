{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPaliwal/CS484Proj/blob/master/cs484%20proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOiQJ0-YBNR8"
      },
      "source": [
        "## CS484 Final Project\n",
        "\n",
        "#### Hardik Paliwal (20725413), Lance Pereira\n",
        "\n",
        "\n",
        "Testing update part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBcYS6VBNSA"
      },
      "source": [
        "For our project we have decided to do Project 7, choosing specifically MNIST with a subset of digits. \n",
        " \n",
        "Our method is .....\n",
        "\n",
        "Our baseline will be a simple CNN of N+1 (with the K extra classes being labelled as \"unknown\") to differentiate between unlabeled and labeled classes. Then simply run (some unsupervised model) on the unlabeled data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phv5gD4DBNSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import sklearn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umKKVD1LBNSE"
      },
      "outputs": [],
      "source": [
        "dev=torch.device(\"cuda\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpBIjSHXBNSF"
      },
      "outputs": [],
      "source": [
        "trainset = tv.datasets.MNIST(root=\"./\", download=True,train=True,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = tv.datasets.MNIST(root=\"./\", download=True,train=False,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhtsaQHSBNSH"
      },
      "outputs": [],
      "source": [
        "#Modifies dataset in place to only have values correspounding to the labels in classesToUse\n",
        "def extractClasses(dataset, classesToUse):\n",
        "    idx = dataset.targets == classesToUse[0]\n",
        "    for k in classesToUse:\n",
        "        idx = (idx | (dataset.targets ==k))\n",
        "    dataset.targets = dataset.targets[idx]\n",
        "    dataset.data = dataset.data[idx]\n",
        "    return dataset\n",
        "\n",
        "#modifies dataset to change classesToSet labels to -1 (which we will assume stands for unknown class)\n",
        "def setClassToUnknown(dataset, classesToSet):\n",
        "  idx = dataset.targets == classesToSet[0]\n",
        "  for k in classesToSet:\n",
        "      idx = (idx | (dataset.targets ==k))\n",
        "  dataset.targets[idx] = -1\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfzjkgNwBNSI"
      },
      "outputs": [],
      "source": [
        "#This is an implementation of VGG11 (which is a precursor to VGG16) for mnist dataset.\n",
        "# it also takes in n, which is the number of classes. N+1 class stands for unknown. \n",
        "\n",
        "#this will let us differeniate the unlabelled data from the labelled data, but I am not yet sure how to train on the unlabelled data. More research is needed\n",
        "class BasicNet(nn.Module):\n",
        "    def __init__(self, n=9):\n",
        "        super(BasicNet, self).__init__()\n",
        "        self.batchNorm = [nn.BatchNorm2d(64), nn.BatchNorm2d(128),nn.BatchNorm2d(256), nn.BatchNorm2d(256),\n",
        "                          nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512)]\n",
        "        self.conv = [\n",
        "        nn.Conv2d(1, 64, 3, 1, 1) ,nn.Conv2d(64, 128, 3, 1, 1), nn.Conv2d(128, 256, 3, 1, 1), nn.Conv2d(256, 256, 3, 1, 1)\n",
        "       ,nn.Conv2d(256, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        ]\n",
        "        maxPool = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv1 = nn.Sequential(self.conv[0], self.batchNorm[0], nn.ReLU(), maxPool)\n",
        "        self.conv2 = nn.Sequential(self.conv[1], self.batchNorm[1], nn.ReLU(), maxPool)\n",
        "        self.conv3 = nn.Sequential(self.conv[2], self.batchNorm[2], nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(self.conv[3], self.batchNorm[3], nn.ReLU(), maxPool) \n",
        "        self.conv5 = nn.Sequential(self.conv[4], self.batchNorm[4], nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(self.conv[5], self.batchNorm[5], nn.ReLU(), maxPool)\n",
        "        self.conv7 = nn.Sequential(self.conv[6], self.batchNorm[6], nn.ReLU()) \n",
        "        self.conv8 = nn.Sequential(self.conv[7], self.batchNorm[7], nn.ReLU(), maxPool)\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, n+1)\n",
        "        \n",
        "    def forward(self, x, feature_embedding=False):\n",
        "        dropOut = nn.Dropout(p=0.5)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = dropOut(F.relu(self.fc1(x)))\n",
        "        x = dropOut(F.relu(self.fc2(x)))\n",
        "        if(feature_embedding):\n",
        "          return x\n",
        "        #Not sure why this works without softmax. probably a reasoning givin in the paper (cause the output can range from anything (not normalized to a 0-1 probability range))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZUIoIjzBNSL"
      },
      "outputs": [],
      "source": [
        "def test(data, net):\n",
        "    net.eval()\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(data):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            test_pred = net(images)\n",
        "\n",
        "            pred = torch.max(test_pred, 1)[1].data.squeeze()\n",
        "            total_correct+= (pred == labels).sum().item()\n",
        "            loss = loss_func( test_pred, labels)\n",
        "            total_loss+= loss.item()*images.size(0)\n",
        "        return total_correct/len(data.dataset), total_loss/len(data.dataset)\n",
        "  \n",
        "def train(num_epochs, net, trainloader, testloader):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    accuracy_through_epochs = []\n",
        "    total_step = len(trainloader)\n",
        "    # val_accuracy, val_loss = test(testloader, net)\n",
        "    # train_accuracy, train_loss = test(trainloader, net)\n",
        "    # accuracy_through_epochs.append([0, val_accuracy, val_loss, train_accuracy, train_loss])\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            optimizer.zero_grad()           \n",
        "            prediction = net(images)\n",
        "            loss = loss_func( prediction, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if ((i +1) % 100 == 0):\n",
        "                print(f\"Epoch {epoch+1} / {num_epochs}, Step {i+1}/ {total_step} , Loss {loss.item()}\")\n",
        "\n",
        "    # val_accuracy, val_loss = test(testloader, net)\n",
        "    # train_accuracy, train_loss = test(trainloader, net)\n",
        "    # accuracy_through_epochs.append([1, val_accuracy, val_loss, train_accuracy, train_loss])\n",
        "    return accuracy_through_epochs, net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4oP9MOBNSN",
        "outputId": "0157c596-8b65-45eb-9cad-2176e26bef60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 4, Step 100/ 469 , Loss 0.2148214727640152\n",
            "Epoch 1 / 4, Step 200/ 469 , Loss 0.11427590250968933\n",
            "Epoch 1 / 4, Step 300/ 469 , Loss 0.07010903209447861\n",
            "Epoch 1 / 4, Step 400/ 469 , Loss 0.07104330509901047\n",
            "Epoch 2 / 4, Step 100/ 469 , Loss 0.013142481446266174\n",
            "Epoch 2 / 4, Step 200/ 469 , Loss 0.0743568167090416\n",
            "Epoch 2 / 4, Step 300/ 469 , Loss 0.044961534440517426\n",
            "Epoch 2 / 4, Step 400/ 469 , Loss 0.021334752440452576\n",
            "Epoch 3 / 4, Step 100/ 469 , Loss 0.047827865928411484\n",
            "Epoch 3 / 4, Step 200/ 469 , Loss 0.011834770441055298\n",
            "Epoch 3 / 4, Step 300/ 469 , Loss 0.0044795433059334755\n",
            "Epoch 3 / 4, Step 400/ 469 , Loss 0.011757533065974712\n",
            "Epoch 4 / 4, Step 100/ 469 , Loss 0.0037738229148089886\n",
            "Epoch 4 / 4, Step 200/ 469 , Loss 0.002915103454142809\n",
            "Epoch 4 / 4, Step 300/ 469 , Loss 0.001752797863446176\n",
            "Epoch 4 / 4, Step 400/ 469 , Loss 0.002278829226270318\n"
          ]
        }
      ],
      "source": [
        "net = BasicNet()\n",
        "net.to(dev)\n",
        "result, trained_net = train(4, net, trainloader, testloader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#do this to store the results in 1 numpy array of 10000 images vs like 20 batches of size 128 images. \n",
        "#get memory error when doing it on a batch of size len(trainloader), so we have to combine the results for trainloader\n",
        "testloaderBig = DataLoader(testset, batch_size=len(testset.targets))\n",
        "trainloaderBig = DataLoader(trainset, batch_size=len(testset.targets))\n",
        "\n",
        "featureEmbed = []\n",
        "predictedDigit = []\n",
        "testFeatureEmbed = \"\"\n",
        "testPredictedDigit = \"\"\n",
        "with torch.no_grad():\n",
        "  for i, (images, labels) in enumerate(trainloaderBig):\n",
        "      images= images.to(dev)\n",
        "      labels = labels.to(dev)\n",
        "      featureEmbed.append(net(images, feature_embedding=True).to(\"cpu\").numpy())\n",
        "      pred = net(images)\n",
        "      predictedDigit.append(torch.max(pred, 1)[1].data.squeeze().to(\"cpu\").numpy())\n",
        "\n",
        "  for i, (images, labels) in enumerate(testloaderBig): #only runs once =D\n",
        "      images= images.to(dev)\n",
        "      labels = labels.to(dev)\n",
        "      testFeatureEmbed= net(images, feature_embedding=True).to(\"cpu\").numpy()\n",
        "      pred = net(images)\n",
        "      testPredictedDigit = torch.max(pred, 1)[1].data.squeeze().to(\"cpu\").numpy()\n",
        "\n",
        "#combine them\n",
        "tmp = featureEmbed[0]\n",
        "tmp1 = predictedDigit[0]\n",
        "for i in range(1, len(featureEmbed)):\n",
        "  tmp = np.concatenate((tmp, featureEmbed[i]))\n",
        "  tmp1 = np.concatenate((tmp1, predictedDigit[i]))\n",
        "\n",
        "featureEmbed = tmp\n",
        "predictedDigit = tmp1"
      ],
      "metadata": {
        "id": "Cay1LKG1dCgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsyJRDl2BNSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412fca01-2f4a-4b80-f38c-bfd8d0e771d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniBatchKMeans(n_clusters=30)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "#should change this when we actually drop classes. Currently running with all classes. \n",
        "n=5\n",
        "d=5\n",
        "kmeans = MiniBatchKMeans(n_clusters = n+d+20)\n",
        "kmeans.fit(featureEmbed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to see how well k-means did we can use this supervised method of defining what a cluster is by seting the cluster label as the most common digits in that cluster\n",
        "#unsupervised approaches include: manually selecting class depending on mean image\n",
        "def retrieve_info(cluster_labels,y_train):\n",
        "  reference_labels = {}\n",
        "# For loop to run through each label of cluster label\n",
        "  for i in range(len(np.unique(kmeans.labels_))):\n",
        "    index = np.where(cluster_labels == i,1,0)\n",
        "    num = np.bincount(y_train[index==1]).argmax()\n",
        "    reference_labels[i] = num\n",
        "  return reference_labels"
      ],
      "metadata": {
        "id": "N4J6Y94UyRci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_labels = retrieve_info(kmeans.labels_,trainset.targets.numpy())\n",
        "number_labels = np.zeros(testFeatureEmbed.shape[0]).astype(int)\n",
        "predicted_test = kmeans.predict(testFeatureEmbed)\n",
        "for i in range(testFeatureEmbed.shape[0]):\n",
        "  number_labels[i] = reference_labels[predicted_test[i]]"
      ],
      "metadata": {
        "id": "xwS6S-FokaZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BUT Note our 30 classes for k-means and the use of the supervised cluster classification. We only get around 0.5 with 10 classes.\n",
        "#also note that I get varying results, from 0.9 to 0.99 when running on 30 clusters.\n",
        "target_test = testset.targets.numpy()\n",
        "accuracy = np.sum(np.where(number_labels == target_test, 1, 0)) / target_test.shape[0]\n",
        "print(f\"Our accuracy with kmeans is: {accuracy} on test set from training on trainset\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVMCQKfNkb38",
        "outputId": "f975da7d-5a2b-4309-e676-1b28d358260c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with kmeans is: 0.9914 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VG-tWrR9whmi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "cs484 proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}