{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPaliwal/CS484Proj/blob/master/cs484%20proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOiQJ0-YBNR8"
      },
      "source": [
        "# **CS484 Final Project**\n",
        "\n",
        "#### Topic 6: Weakly supervised classification\n",
        "\n",
        "#### Hardik Paliwal (20725413), Lance Pereira (20719626)\n",
        "\n",
        "______________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Table Of Contents**\n",
        "- A) Abstract\n",
        "- B) High Level Goals and Methedology\n",
        "- C) Team Members and Contributions\n",
        "- D) External Code Libraries\n",
        "- E) Code\n",
        "  - 1) Setup (imports, loading data)\n",
        "  - 2) Define our Base CNN (Based on VGG11), Test, Train methods\n",
        "  - 3) Split Fashion MNIST training data into M labeled images, and N-M unlabeled\n",
        "  - 4) Train Base CNN on M labeled training images\n",
        "  - 5) Use the 'semi' trained CNN to gain important features (feature embedings) of N-M *unlabeled* training images\n",
        "  - 6) Code: Assign labels to unlabeled images\n",
        "  - 7) Define retrained CNN function using predicted labels from clustering for unlabeled data\n",
        "- F) Experiments\n",
        "  - 1) Diffirent ratios of M/N\n",
        "  - 2) Diffirent Clustering Methods\n",
        "- G) Results\n",
        "  - 1) Results of diffirent ratios\n",
        "  - 2) Results from diffirent clustering methods\n",
        "  - 3) Results from Clustering vs retrained CNN\n",
        "- H) Conclusions"
      ],
      "metadata": {
        "id": "P5a9RiUbRKX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBcYS6VBNSA"
      },
      "source": [
        "#**A) Abstract:**\n",
        "\n",
        "For our project we have decided to do Project 6, choosing specifically Fashion MNIST. We use weakly supervised classification to try and improve results compared to just using supervised learning.\n",
        " \n",
        "\n",
        "#**B) High Level Goals and Methedology:**\n",
        "\n",
        "Out high level goal is to use weakly supervised classification to improve our \n",
        "prediction ability compared to just training using labeled images. We hope to \n",
        "achieve atleast a 5% increase in our test prediction score using clustering \n",
        "methods.\n",
        "\n",
        "Our method is split into two experiments, \n",
        "- we first will test to see training\n",
        "a CNN on N-M labeled images, then use a cluestering method to classify all the \n",
        "N images, using the majority label in each cluster as the predicted label\n",
        "- secondly we will try using the predicted labels from the previous step to \n",
        "train a new CNN model, to see if it performs better\n",
        "\n",
        "Our baseline will be a simple CNN of N+1 (with the K extra classes being labelled as \"unknown\") to differentiate between unlabeled and labeled classes. Then simply run (some unsupervised model) on the unlabeled data. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**C) Team Members and Contributions**\n",
        "\n",
        "- Hardik Paliwal\n",
        "  - Created CNN based on VGG11 for training\n",
        "  - Created function to gather features from pretrained CNN (trained on M labeled images)\n",
        "  - Created method to get predicted labels from clustering methods\n",
        "\n",
        "- Lance Pereira\n",
        "  - (TODO) Modified VGG11 like CNN for better results\n",
        "  - Created experiments for ratios of M/N \n",
        "  - Split data into labeled and unlabeled\n",
        "  - Created experiments for diffirent clustering methods\n",
        "    - Kmeans, Kmedians, Kmodes\n",
        "    - GMM\n",
        "  - Modified the Clustering method labeler to work with unlabeled data and labeled data\n",
        "  - Create function to get retrained CNN using clustered labels"
      ],
      "metadata": {
        "id": "3ZbWhkZiOTHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**D) External Code Libraries**\n",
        "\n",
        "We used\n",
        "\n",
        "- Pytorch\n",
        "  - Because it was crucial for the quick training of our models\n",
        "  - Allowed us to not have to deal with calculating back propogation\n",
        "- Sklearn\n",
        "  - Provided us a large range of clustering methods for quick experimentation\n",
        "- Numpy\n",
        "  - Useful for large matrix operations"
      ],
      "metadata": {
        "id": "YHKO718hQ17r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "phv5gD4DBNSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import sklearn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E) Code**"
      ],
      "metadata": {
        "id": "y_GC_Ou-aqJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [1] Code: Setup**\n",
        "\n",
        "The below cells:\n",
        "- Import Fshion MNIST\n",
        "- define functions that strip label from data"
      ],
      "metadata": {
        "id": "ev3-3ZQobP0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "umKKVD1LBNSE"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "dev=torch.device(\"cuda\") \n",
        "NUM_EPOCHS = 6\n",
        "NUM_CLUSTERS = 30\n",
        "NUM_CLASSES = 10\n",
        "UNKNOWN_CLASS = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lpBIjSHXBNSF"
      },
      "outputs": [],
      "source": [
        "trainset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=True,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=False,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [2] Code: Define our Base CNN (Based on VGG11), Test, Train methods**"
      ],
      "metadata": {
        "id": "o24XBil8bahy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sfzjkgNwBNSI"
      },
      "outputs": [],
      "source": [
        "#This is an implementation of VGG11 (which is a precursor to VGG16) for mnist dataset.\n",
        "# it also takes in n, which is the number of classes. N+1 class stands for unknown. \n",
        "\n",
        "#this will let us differeniate the unlabelled data from the labelled data\n",
        "class BasicNet(nn.Module):\n",
        "    def __init__(self, n=9):\n",
        "        super(BasicNet, self).__init__()\n",
        "        self.batchNorm = [nn.BatchNorm2d(64), nn.BatchNorm2d(128),nn.BatchNorm2d(256), nn.BatchNorm2d(256),\n",
        "                          nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512)]\n",
        "        self.conv = [\n",
        "        nn.Conv2d(1, 64, 3, 1, 1) ,nn.Conv2d(64, 128, 3, 1, 1), nn.Conv2d(128, 256, 3, 1, 1), nn.Conv2d(256, 256, 3, 1, 1)\n",
        "       ,nn.Conv2d(256, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        ]\n",
        "        maxPool = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv1 = nn.Sequential(self.conv[0], self.batchNorm[0], nn.ReLU(), maxPool)\n",
        "        self.conv2 = nn.Sequential(self.conv[1], self.batchNorm[1], nn.ReLU(), maxPool)\n",
        "        self.conv3 = nn.Sequential(self.conv[2], self.batchNorm[2], nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(self.conv[3], self.batchNorm[3], nn.ReLU(), maxPool) \n",
        "        self.conv5 = nn.Sequential(self.conv[4], self.batchNorm[4], nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(self.conv[5], self.batchNorm[5], nn.ReLU(), maxPool)\n",
        "        self.conv7 = nn.Sequential(self.conv[6], self.batchNorm[6], nn.ReLU()) \n",
        "        self.conv8 = nn.Sequential(self.conv[7], self.batchNorm[7], nn.ReLU(), maxPool)\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, n+1)\n",
        "        \n",
        "    def forward(self, x, feature_embedding=False):\n",
        "        dropOut = nn.Dropout(p=0.5)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = dropOut(F.relu(self.fc1(x)))\n",
        "        x = dropOut(F.relu(self.fc2(x)))\n",
        "        if(feature_embedding):\n",
        "          return x\n",
        "        #Not sure why this works without softmax. probably a reasoning givin in the paper (cause the output can range from anything (not normalized to a 0-1 probability range))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7ZUIoIjzBNSL"
      },
      "outputs": [],
      "source": [
        "def test(data, net):\n",
        "    net.eval()\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(data):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            test_pred = net(images)\n",
        "\n",
        "            pred = torch.max(test_pred, 1)[1].data.squeeze()\n",
        "            total_correct+= (pred == labels).sum().item()\n",
        "            loss = loss_func( test_pred, labels)\n",
        "            total_loss+= loss.item()*images.size(0)\n",
        "        # return total_correct/len(data.dataset), total_loss/len(data.dataset)\n",
        "        # Note From Lance: We shouldn't divide the total loss\n",
        "        return total_correct/len(data.dataset), total_loss\n",
        "  \n",
        "def train(num_epochs, net, trainloader):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    accuracy_through_epochs = []\n",
        "    total_step = len(trainloader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            optimizer.zero_grad()           \n",
        "            prediction = net(images)\n",
        "            loss = loss_func( prediction, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if ((i +1) % 100 == 0):\n",
        "                print(f\"Epoch {epoch+1} / {num_epochs}, Step {i+1}/ {total_step} , Loss {loss.item()}\")\n",
        "\n",
        "    return accuracy_through_epochs, net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [3] Code: Split Fashion MNIST training data into M labeled images, and N-M unlabeled**"
      ],
      "metadata": {
        "id": "NK6JVE0UGfF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modifies dataset in place to only have values correspounding to the labels in classesToUse\n",
        "def splitTrainingData(training_data, M_percent):\n",
        "  # N is len(training_data)\n",
        "  len_N = len(trainset)\n",
        "\n",
        "  # M is the number of labeled images we want\n",
        "  len_M = int(M_percent*len_N)\n",
        "  \n",
        "  labeled_data, unlabeled_data = torch.utils.data.random_split(trainset, [len_M, len_N - len_M])\n",
        "\n",
        "  # strip the labels from unlabeled_data\n",
        "  # unlabeled_data.dataset.targets[unlabeled_data.indices] = UNKNOWN_CLASS\n",
        "\n",
        "  labeled_data_loader = DataLoader(labeled_data, batch_size=128, shuffle=True)\n",
        "  unlabeled_data_loader = DataLoader(unlabeled_data, batch_size=128, shuffle=True)\n",
        "\n",
        "  return labeled_data_loader, unlabeled_data_loader, labeled_data, unlabeled_data\n"
      ],
      "metadata": {
        "id": "HaaxUxtcGppA"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_train_loader, unlabeled_train_loader, labeled_train_data, unlabeled_train_data = splitTrainingData(trainset, 0.7)"
      ],
      "metadata": {
        "id": "GaNdStKMkQun"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [4] Code: Train Base CNN on M labeled training images**"
      ],
      "metadata": {
        "id": "iPnEeZO5chxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4oP9MOBNSN",
        "outputId": "d7394a57-b503-420c-c9a2-beb8802c07a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 329 , Loss 0.563522219657898\n",
            "Epoch 1 / 6, Step 200/ 329 , Loss 0.3923516571521759\n",
            "Epoch 1 / 6, Step 300/ 329 , Loss 0.4406273365020752\n",
            "Epoch 2 / 6, Step 100/ 329 , Loss 0.3055887818336487\n",
            "Epoch 2 / 6, Step 200/ 329 , Loss 0.21421709656715393\n",
            "Epoch 2 / 6, Step 300/ 329 , Loss 0.1780194640159607\n",
            "Epoch 3 / 6, Step 100/ 329 , Loss 0.4110018312931061\n",
            "Epoch 3 / 6, Step 200/ 329 , Loss 0.3142193555831909\n",
            "Epoch 3 / 6, Step 300/ 329 , Loss 0.21675387024879456\n",
            "Epoch 4 / 6, Step 100/ 329 , Loss 0.27567151188850403\n",
            "Epoch 4 / 6, Step 200/ 329 , Loss 0.20697882771492004\n",
            "Epoch 4 / 6, Step 300/ 329 , Loss 0.18259502947330475\n",
            "Epoch 5 / 6, Step 100/ 329 , Loss 0.11308476328849792\n",
            "Epoch 5 / 6, Step 200/ 329 , Loss 0.19030247628688812\n",
            "Epoch 5 / 6, Step 300/ 329 , Loss 0.10715696960687637\n",
            "Epoch 6 / 6, Step 100/ 329 , Loss 0.18783345818519592\n",
            "Epoch 6 / 6, Step 200/ 329 , Loss 0.1297719031572342\n",
            "Epoch 6 / 6, Step 300/ 329 , Loss 0.10071364790201187\n"
          ]
        }
      ],
      "source": [
        "net = BasicNet()\n",
        "net.to(dev)\n",
        "result, trained_net = train(NUM_EPOCHS, net, labeled_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [5] Code: Use the 'semi' trained CNN to gain important features (feature embedings) of N-M *unlabeled* training images**"
      ],
      "metadata": {
        "id": "JgorThJ9GTSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#do this to store the results in 1 numpy array of 10000 images vs like 20 batches of size 128 images. \n",
        "#get memory error when doing it on a batch of size len(trainloader), so we have to combine the results for trainloader\n",
        "\n",
        "def getFeatureEmbedings(dataloader):\n",
        "  featureEmbed = []\n",
        "  predictedDigit = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images= images.to(dev)\n",
        "        labels = labels.to(dev)\n",
        "        featureEmbed.append(net(images, feature_embedding=True).to(\"cpu\").numpy())\n",
        "        pred = net(images)\n",
        "        predictedDigit.append(torch.max(pred, 1)[1].data.squeeze().to(\"cpu\").numpy())\n",
        "\n",
        "  # flatten lists\n",
        "  featureEmbed = np.array(list(itertools.chain(*featureEmbed)))\n",
        "  predictedDigit = np.array(list(itertools.chain(*predictedDigit)))\n",
        "\n",
        "  return featureEmbed, predictedDigit"
      ],
      "metadata": {
        "id": "Cay1LKG1dCgt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need the data with the labels\n",
        "trainFeatureEmbed, trainPredictedDigit = getFeatureEmbedings(trainloader)"
      ],
      "metadata": {
        "id": "RXoE9MVsncaB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [6] Code: Assign labels to unlabeled images**"
      ],
      "metadata": {
        "id": "cP92A4A4FnBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to see how well k-means did we can use this supervised method of defining what a cluster is by seting the cluster label as the most common digits in that cluster\n",
        "#unsupervised approaches include: manually selecting class depending on mean image\n",
        "def retrieve_cluster_to_classification(cluster_labels,y_train):\n",
        "  reference_labels = {}\n",
        "# For loop to run through each label of cluster label\n",
        "  for i in range(len(np.unique(kmeans.labels_))):\n",
        "    index = np.where(cluster_labels == i,1,0)\n",
        "    # we only read 0:NUM_CLASSES so we dont read the unknown labels\n",
        "    num = np.bincount(y_train[index==1])[:NUM_CLASSES].argmax()\n",
        "    reference_labels[i] = num\n",
        "    # TODO: Right now the refrence label just maps to the majority label, should we also consider the 2nd and 3rd highest\n",
        "  return reference_labels"
      ],
      "metadata": {
        "id": "N4J6Y94UyRci"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [7] Code: Define retrained CNN function using predicted labels from clustering for unlabeled data**\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ91J2ujyL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_CNN_with_predicted_labels(train_dataset, my_labeled_trainset, my_unlabeled_trainset, unlabeled_train_loader, cluster_method):\n",
        "  \n",
        "  # remove all unlabeled labels from main training set, set them to NUM_CLASSES\n",
        "  train_targets = np.array(train_dataset.targets.numpy(), copy=True)  \n",
        "  train_targets[my_unlabeled_trainset.indices] = NUM_CLASSES\n",
        "  \n",
        "  # mapping from NUM_CLUSTERS to FASHION_MNIST classes\n",
        "  reference_labels = retrieve_cluster_to_classification(\n",
        "      cluster_method.labels_, \n",
        "      train_targets\n",
        "  )\n",
        "\n",
        "  unlabelTrainFeatureEmbed, _ = getFeatureEmbedings(unlabeled_train_loader)\n",
        "  \n",
        "  # Assign new labels\n",
        "  predicted_test = cluster_method.predict(unlabelTrainFeatureEmbed)\n",
        "  \n",
        "  for i in range(unlabelTrainFeatureEmbed.shape[0]):\n",
        "    my_unlabeled_trainset.dataset.targets[my_unlabeled_trainset.indices[i]] = reference_labels[predicted_test[i]]\n",
        "\n",
        "  new_combined_data = torch.utils.data.ConcatDataset([my_labeled_trainset, my_unlabeled_trainset])\n",
        "  new_combined_data_loader = DataLoader(new_combined_data, batch_size=128, shuffle=True)\n",
        "  retrained_net = BasicNet()\n",
        "  retrained_net.to(dev)\n",
        "  result, retrained_net = train(NUM_EPOCHS, retrained_net, new_combined_data_loader)\n",
        "  return retrained_net\n",
        "\n"
      ],
      "metadata": {
        "id": "xwS6S-FokaZJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Experiments**\n"
      ],
      "metadata": {
        "id": "KjveyCWCmUnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment A) First we will create three experiments using diffirent ratios of M:N-M\n",
        "\n",
        "\n",
        "1.   Ratio of 70% labeled to 30% unlabeled\n",
        "2.   Ratio of 50% labeled to 50% unlabeled\n",
        "3.   Ratio of 30% labeled to 70% unlabeled\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zk9ebry0n4lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment A: Diffirent Ratios of M to N\n",
        "\n",
        "labeled_train_loader_70, unlabeled_train_loader_30, labeled_train_data_70, unlabeled_train_data_30 = splitTrainingData(trainset, 0.7)\n",
        "labeled_train_loader_50, unlabeled_train_loader_50, labeled_train_data_50, unlabeled_train_data_50 = splitTrainingData(trainset, 0.5)\n",
        "labeled_train_loader_30, unlabeled_train_loader_70, labeled_train_data_30, unlabeled_train_data_70 = splitTrainingData(trainset, 0.3)"
      ],
      "metadata": {
        "id": "njxlymmEn6UR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment B) Then we will try training it with diffirent clustering methods"
      ],
      "metadata": {
        "id": "gU2k8HEDrNYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "kmeans = MiniBatchKMeans(n_clusters = NUM_CLUSTERS)\n",
        "\n",
        "kmeans.fit(trainFeatureEmbed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAss2NNi-6-z",
        "outputId": "d4ba8d3e-6b80-4633-99a7-7edcf7da50cd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniBatchKMeans(n_clusters=30)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "gmm = DBSCAN(eps=3, min_samples= NUM_CLUSTERS)\n",
        "\n",
        "gmm.fit(trainFeatureEmbed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYIOUEK-wo9",
        "outputId": "fdb2b3b9-0092-479e-e56a-d8a640cad6b4"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DBSCAN(eps=3, min_samples=30)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DbrICQpoeAh8"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment C) Train all the models combining diffirent experiments"
      ],
      "metadata": {
        "id": "8dpNhodXA5pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_70_label_30_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_70, unlabeled_train_data_30, unlabeled_train_loader_30, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v01rGfhXBFaU",
        "outputId": "6ff5c782-5bdb-45b2-b832-bf7780c833cf"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 1.5237332582473755\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 1.2228803634643555\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 1.2317708730697632\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 1.4264209270477295\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 1.4124491214752197\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 1.3803890943527222\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 1.104261040687561\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 1.4154531955718994\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 1.2753548622131348\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 1.270864725112915\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 1.2200355529785156\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 1.3213220834732056\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 1.0486948490142822\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 1.1233041286468506\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 1.1874897480010986\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.9088383316993713\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 1.0206546783447266\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 1.306497573852539\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 1.0899863243103027\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 1.2969001531600952\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 1.1225556135177612\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 1.1610090732574463\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 1.2202436923980713\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 1.2348406314849854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_50_label_50_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_50, unlabeled_train_data_50, unlabeled_train_loader_50, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz1hF8lNBHR_",
        "outputId": "8c4e9577-87f8-4136-a11f-3334494db686"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 1.3262841701507568\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 1.1499066352844238\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 1.3281002044677734\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 1.1707172393798828\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 1.1296488046646118\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 1.1957318782806396\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.9648768901824951\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 1.208524465560913\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 1.0582894086837769\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 1.2255021333694458\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 1.0864713191986084\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 1.1648823022842407\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 1.0826817750930786\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 1.306212306022644\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.9369737505912781\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.9683341979980469\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 1.0869994163513184\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 1.0911811590194702\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 1.2174482345581055\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 1.1271660327911377\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 1.239988088607788\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 1.0701022148132324\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 1.113822102546692\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.9992640614509583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_30_label_70_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_30, unlabeled_train_data_70, unlabeled_train_loader_70, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjNFDCvOBHam",
        "outputId": "4ed47a57-64b0-47dc-a457-4c9906efd790"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 0.7395923137664795\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 0.49900320172309875\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 0.6023542284965515\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 0.597287118434906\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 0.767909586429596\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 0.5157312154769897\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.4789220690727234\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 0.333807110786438\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.4427388608455658\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 0.5311180949211121\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 0.509806215763092\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.7157886624336243\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.63088059425354\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 0.472177654504776\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.6406874060630798\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.46609607338905334\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 0.5201724767684937\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 0.4584900438785553\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.5295698046684265\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.42284783720970154\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.37964123487472534\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 0.2653249204158783\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.3866492509841919\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.4927137494087219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_70_label_30_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_70, unlabeled_train_data_30, unlabeled_train_loader_30, gmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "0-i9wlc7BMDr",
        "outputId": "0f3c75c0-c3aa-44cb-c8a0-db981d31aabe"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-9577f08428d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretrained_net_gmm_70_label_30_not\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_CNN_with_predicted_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_train_data_70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_train_data_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabeled_train_loader_30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-594ccfaa311f>\u001b[0m in \u001b[0;36mretrain_CNN_with_predicted_labels\u001b[0;34m(train_dataset, my_labeled_trainset, my_unlabeled_trainset, unlabeled_train_loader, cluster_method)\u001b[0m\n\u001b[1;32m      8\u001b[0m   reference_labels = retrieve_cluster_to_classification(\n\u001b[1;32m      9\u001b[0m       \u001b[0mcluster_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mtrain_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   )\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-0a4f5f9dabdc>\u001b[0m in \u001b[0;36mretrieve_cluster_to_classification\u001b[0;34m(cluster_labels, y_train)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# we only read 0:NUM_CLASSES so we dont read the unknown labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreference_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# TODO: Right now the refrence label just maps to the majority label, should we also consider the 2nd and 3rd highest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(gmm.labels_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jSLI-DFBujs",
        "outputId": "f79dd5a5-2cb1-4e81-a585-49c14c64d208"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_50_label_50_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_50, unlabeled_train_data_50, unlabeled_train_loader_50, gmm)"
      ],
      "metadata": {
        "id": "CSar1kepBMJw"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_30_label_70_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_30, unlabeled_train_data_70, unlabeled_train_loader_70, gmm)"
      ],
      "metadata": {
        "id": "XM5MAfTbBMTA"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results**\n"
      ],
      "metadata": {
        "id": "JGBSP5TXBiGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First we will look at the **baseline** results from the CNN trained on M labeled images"
      ],
      "metadata": {
        "id": "fnVfGL5oZU3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testFeatureEmbed, _ = getFeatureEmbedings(testloader)"
      ],
      "metadata": {
        "id": "9IKInqSi6C1S"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_accuracy, loss = test(testloader, net)\n",
        "print(f\"Our accuracy with just the CNN is: {baseline_accuracy} on test set from training on trainset\" )"
      ],
      "metadata": {
        "id": "pH2XADkWZSER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5c1d9a-d306-42a2-8ef1-6dce7681ef8e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with just the CNN is: 0.8524 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next we will look at all the other methods that use weakly supervised learning, we aim to be better than the baseline in each\n"
      ],
      "metadata": {
        "id": "vJ2rkCOdZUPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_70_label_30_not, loss = test(testloader, retrained_net_kmeans_70_label_30_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 70% labeled, 30% unlabeled is: {accuracy_kmeans_70_label_30_not} on test set\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yut5RI8zAxZg",
        "outputId": "0ed7d789-35d3-436f-afb3-922a5bafc4ae"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 70% labeled, 30% unlabeled is: 0.8694 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_50_label_50_not, loss = test(testloader, retrained_net_kmeans_50_label_50_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 50% labeled, 50% unlabeled is: {accuracy_kmeans_50_label_50_not} on test set\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0d8xRVIAxlT",
        "outputId": "91c48f7c-b276-48a5-d2e0-cdf051e18bf6"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 50% labeled, 50% unlabeled is: 0.1273 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_30_label_70_not, loss = test(testloader, retrained_net_kmeans_30_label_70_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 30% labeled, 70% unlabeled is: {accuracy_kmeans_30_label_70_not} on test set\" )"
      ],
      "metadata": {
        "id": "IvzHxZpzC9tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236bc157-a073-4645-8b0a-688aaa03976c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 30% labeled, 70% unlabeled is: 0.1 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy_gmm_70_label_30_not, loss = test(testloader, retrained_net_gmm_70_label_30_not)\n",
        "# print(f\"Our accuracy with the CNN and GMM clustering with 70% labeled, 30% unlabeled is: {accuracy_gmm_70_label_30_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "id": "0V2K0kkdCjJ6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy_gmm_50_label_50_not, loss = test(testloader, retrained_net_gmm_50_label_50_not)\n",
        "# print(f\"Our accuracy with the CNN and GMM clustering with 50% labeled, 50% unlabeled is: {accuracy_gmm_50_label_50_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "id": "CmVV-eFACjUc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy_gmm_30_label_70_not, loss = test(testloader, retrained_net_gmm_30_label_70_not)\n",
        "# print(f\"Our accuracy with the CNN and GMM clustering with 30% labeled, 70% unlabeled is: {accuracy_gmm_30_label_70_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "id": "Q1zSeky8Cjbu"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusions**\n",
        "\n",
        "- We found that the most useful ratio of M/N which was 30% labeled, 70% unlabeled yieled\n",
        "- This compared to 50:50\n",
        "- Compared to 70% labeled, 30% unlabeled"
      ],
      "metadata": {
        "id": "1GxL84V_Rv82"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of cs484 proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}