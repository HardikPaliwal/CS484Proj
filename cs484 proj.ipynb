{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPaliwal/CS484Proj/blob/master/cs484%20proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOiQJ0-YBNR8"
      },
      "source": [
        "# **CS484 Final Project**\n",
        "\n",
        "#### Topic 6: Weakly supervised classification\n",
        "\n",
        "#### Hardik Paliwal (20725413), Lance Pereira (20719626)\n",
        "\n",
        "______________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Table Of Contents**\n",
        "- A) Abstract\n",
        "- B) High Level Goals and Methedology\n",
        "- C) Team Members and Contributions\n",
        "- D) External Code Libraries\n",
        "- E) Code\n",
        "  - 1) Setup (imports, loading data)\n",
        "  - 2) Define our Base CNN (Based on VGG11), Test, Train methods\n",
        "  - 3) Split Fashion MNIST training data into M labeled images, and N-M unlabeled\n",
        "  - 4) Train Base CNN on M labeled training images\n",
        "  - 5) Use the 'semi' trained CNN to gain important features (feature embedings) of N-M *unlabeled* training images\n",
        "  - 6) Match cluster labels to actual Fashion MNIST labels\n",
        "  - 7) Define retrained CNN function using predicted labels from clustering for unlabeled data\n",
        "- F) Experiments\n",
        "  - 1) Diffirent ratios of M/N\n",
        "  - 2) Diffirent Clustering Methods\n",
        "- G) Results\n",
        "- H) Conclusions"
      ],
      "metadata": {
        "id": "P5a9RiUbRKX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBcYS6VBNSA"
      },
      "source": [
        "#**A) Abstract:**\n",
        "\n",
        "For our project we have decided to do Project 6, choosing specifically Fashion MNIST. We use weakly supervised classification using feature extraction (embedings similar to a PCA) along with clustering methods to try and improve results compared to just using supervised learning.\n",
        " \n",
        "\n",
        "#**B) High Level Goals and Methedology:**\n",
        "\n",
        "Out high level goal is to use weakly supervised classification to improve our \n",
        "prediction ability compared to just training using labeled images. We hope to \n",
        "achieve atleast a 5% increase in our test prediction score using clustering \n",
        "methods such as Kmeans and Mini-Batch Kmeans. Our attempts to use other clustering methods were stopped by our limited RAM capacity, but Kmeans and Mini-Batch Kmeans are quite effective.\n",
        "\n",
        "Our method is split into two experiments, \n",
        "- we first will test to see training\n",
        "a CNN on M labeled images, then use a cluestering method to classify all the \n",
        "N images, using the majority label in each cluster as the predicted label for the unlabeled N-M images\n",
        "- secondly we will try using the predicted labels from the previous step to \n",
        "train a new CNN model, to see if it performs better than the original CNN model trained on just the M labeled images\n",
        "\n",
        "Our baseline will be a simple CNN  trained on the M labeled images. We will use this to see if our weakly supervised method improved the accuracy rate. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**C) Team Members and Contributions**\n",
        "\n",
        "- Hardik Paliwal\n",
        "  - Created CNN based on VGG11 for training\n",
        "  - Created function to gather features (feature embedings) from pretrained CNN (trained on M labeled images)\n",
        "  - Created method to get predicted labels from clustering methods\n",
        "  - Created train,test methods\n",
        "\n",
        "- Lance Pereira\n",
        "  - Create function to retrain CNN using clustered labels\n",
        "  - Modified clustering prediction methods to use only labeled training data\n",
        "  - Created experiments for diffirent ratios of M:M-N\n",
        "  - Created function to split data into labeled and unlabeled\n",
        "  - Created experiments for diffirent clustering methods\n",
        "    - Mini Batch Kmeans\n",
        "    - Kmeans\n",
        "  - Wrote/formated report\n",
        "\n"
      ],
      "metadata": {
        "id": "3ZbWhkZiOTHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**D) External Code Libraries**\n",
        "\n",
        "We used\n",
        "\n",
        "- Pytorch\n",
        "  - Because it was crucial for the quick training of our models\n",
        "  - Allowed us to not have to deal with calculating back propogation\n",
        "- Sklearn\n",
        "  - Provided us a large range of clustering methods for quick experimentation\n",
        "- Numpy\n",
        "  - Useful for large matrix operations"
      ],
      "metadata": {
        "id": "YHKO718hQ17r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "phv5gD4DBNSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import sklearn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E) Code**"
      ],
      "metadata": {
        "id": "y_GC_Ou-aqJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [1] Code: Setup**\n",
        "\n",
        "The below cells:\n",
        "- Import Fshion MNIST\n",
        "- define functions that strip label from data"
      ],
      "metadata": {
        "id": "ev3-3ZQobP0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "umKKVD1LBNSE"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "dev=torch.device(\"cuda\") \n",
        "NUM_EPOCHS = 6\n",
        "NUM_CLUSTERS = 30\n",
        "NUM_CLASSES = 10\n",
        "UNKNOWN_CLASS = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lpBIjSHXBNSF"
      },
      "outputs": [],
      "source": [
        "trainset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=True,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=False,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [2] Code: Define our Base CNN (Based on VGG11), Test, Train methods**"
      ],
      "metadata": {
        "id": "o24XBil8bahy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sfzjkgNwBNSI"
      },
      "outputs": [],
      "source": [
        "#This is an implementation of VGG11 (which is a precursor to VGG16) for mnist dataset.\n",
        "# it also takes in n, which is the number of classes. N+1 class stands for unknown. \n",
        "\n",
        "#this will let us differeniate the unlabelled data from the labelled data\n",
        "class BasicNet(nn.Module):\n",
        "    def __init__(self, n=9):\n",
        "        super(BasicNet, self).__init__()\n",
        "        self.batchNorm = [nn.BatchNorm2d(64), nn.BatchNorm2d(128),nn.BatchNorm2d(256), nn.BatchNorm2d(256),\n",
        "                          nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512)]\n",
        "        self.conv = [\n",
        "        nn.Conv2d(1, 64, 3, 1, 1) ,nn.Conv2d(64, 128, 3, 1, 1), nn.Conv2d(128, 256, 3, 1, 1), nn.Conv2d(256, 256, 3, 1, 1)\n",
        "       ,nn.Conv2d(256, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        ]\n",
        "        maxPool = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv1 = nn.Sequential(self.conv[0], self.batchNorm[0], nn.ReLU(), maxPool)\n",
        "        self.conv2 = nn.Sequential(self.conv[1], self.batchNorm[1], nn.ReLU(), maxPool)\n",
        "        self.conv3 = nn.Sequential(self.conv[2], self.batchNorm[2], nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(self.conv[3], self.batchNorm[3], nn.ReLU(), maxPool) \n",
        "        self.conv5 = nn.Sequential(self.conv[4], self.batchNorm[4], nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(self.conv[5], self.batchNorm[5], nn.ReLU(), maxPool)\n",
        "        self.conv7 = nn.Sequential(self.conv[6], self.batchNorm[6], nn.ReLU()) \n",
        "        self.conv8 = nn.Sequential(self.conv[7], self.batchNorm[7], nn.ReLU(), maxPool)\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, n+1)\n",
        "        \n",
        "    def forward(self, x, feature_embedding=False):\n",
        "        dropOut = nn.Dropout(p=0.5)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = dropOut(F.relu(self.fc1(x)))\n",
        "        x = dropOut(F.relu(self.fc2(x)))\n",
        "        if(feature_embedding):\n",
        "          return x\n",
        "        #Not sure why this works without softmax. probably a reasoning givin in the paper (cause the output can range from anything (not normalized to a 0-1 probability range))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7ZUIoIjzBNSL"
      },
      "outputs": [],
      "source": [
        "def test(data, net):\n",
        "    net.eval()\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(data):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            test_pred = net(images)\n",
        "\n",
        "            pred = torch.max(test_pred, 1)[1].data.squeeze()\n",
        "            total_correct+= (pred == labels).sum().item()\n",
        "            loss = loss_func( test_pred, labels)\n",
        "            total_loss+= loss.item()*images.size(0)\n",
        "        # return total_correct/len(data.dataset), total_loss/len(data.dataset)\n",
        "        # Note From Lance: We shouldn't divide the total loss\n",
        "        return total_correct/len(data.dataset), total_loss\n",
        "  \n",
        "def train(num_epochs, net, trainloader):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    accuracy_through_epochs = []\n",
        "    total_step = len(trainloader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            optimizer.zero_grad()           \n",
        "            prediction = net(images)\n",
        "            loss = loss_func( prediction, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if ((i +1) % 100 == 0):\n",
        "                print(f\"Epoch {epoch+1} / {num_epochs}, Step {i+1}/ {total_step} , Loss {loss.item()}\")\n",
        "\n",
        "    return accuracy_through_epochs, net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [3] Code: Split Fashion MNIST training data into M labeled images, and N-M unlabeled**"
      ],
      "metadata": {
        "id": "NK6JVE0UGfF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modifies dataset in place to only have values correspounding to the labels in classesToUse\n",
        "def splitTrainingData(training_data, M_percent):\n",
        "  # N is len(training_data)\n",
        "  len_N = len(trainset)\n",
        "\n",
        "  # M is the number of labeled images we want\n",
        "  len_M = int(M_percent*len_N)\n",
        "  \n",
        "  labeled_data, unlabeled_data = torch.utils.data.random_split(trainset, [len_M, len_N - len_M])\n",
        "\n",
        "  # strip the labels from unlabeled_data\n",
        "  # unlabeled_data.dataset.targets[unlabeled_data.indices] = UNKNOWN_CLASS\n",
        "\n",
        "  labeled_data_loader = DataLoader(labeled_data, batch_size=128, shuffle=True)\n",
        "  unlabeled_data_loader = DataLoader(unlabeled_data, batch_size=128, shuffle=True)\n",
        "\n",
        "  return labeled_data_loader, unlabeled_data_loader, labeled_data, unlabeled_data\n"
      ],
      "metadata": {
        "id": "HaaxUxtcGppA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_train_loader, unlabeled_train_loader, labeled_train_data, unlabeled_train_data = splitTrainingData(trainset, 0.7)"
      ],
      "metadata": {
        "id": "GaNdStKMkQun"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [4] Code: Train Base CNN on M labeled training images**"
      ],
      "metadata": {
        "id": "iPnEeZO5chxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4oP9MOBNSN",
        "outputId": "5f5553e8-3fa6-4571-98f2-afcfbee01d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 329 , Loss 0.6301531195640564\n",
            "Epoch 1 / 6, Step 200/ 329 , Loss 0.4433915317058563\n",
            "Epoch 1 / 6, Step 300/ 329 , Loss 0.30672404170036316\n",
            "Epoch 2 / 6, Step 100/ 329 , Loss 0.30975407361984253\n",
            "Epoch 2 / 6, Step 200/ 329 , Loss 0.2942114472389221\n",
            "Epoch 2 / 6, Step 300/ 329 , Loss 0.14940808713436127\n",
            "Epoch 3 / 6, Step 100/ 329 , Loss 0.2286154180765152\n",
            "Epoch 3 / 6, Step 200/ 329 , Loss 0.2465631663799286\n",
            "Epoch 3 / 6, Step 300/ 329 , Loss 0.3027246594429016\n",
            "Epoch 4 / 6, Step 100/ 329 , Loss 0.12336543202400208\n",
            "Epoch 4 / 6, Step 200/ 329 , Loss 0.1637580841779709\n",
            "Epoch 4 / 6, Step 300/ 329 , Loss 0.21599192917346954\n",
            "Epoch 5 / 6, Step 100/ 329 , Loss 0.12565259635448456\n",
            "Epoch 5 / 6, Step 200/ 329 , Loss 0.2050739824771881\n",
            "Epoch 5 / 6, Step 300/ 329 , Loss 0.12265955656766891\n",
            "Epoch 6 / 6, Step 100/ 329 , Loss 0.11220794171094894\n",
            "Epoch 6 / 6, Step 200/ 329 , Loss 0.10667518526315689\n",
            "Epoch 6 / 6, Step 300/ 329 , Loss 0.18989360332489014\n"
          ]
        }
      ],
      "source": [
        "net = BasicNet()\n",
        "net.to(dev)\n",
        "result, trained_net = train(NUM_EPOCHS, net, labeled_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [5] Code: Use the 'semi' trained CNN to gain important features (feature embedings) of N-M *unlabeled* training images**"
      ],
      "metadata": {
        "id": "JgorThJ9GTSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#do this to store the results in 1 numpy array of 10000 images vs like 20 batches of size 128 images. \n",
        "#get memory error when doing it on a batch of size len(trainloader), so we have to combine the results for trainloader\n",
        "\n",
        "def getFeatureEmbedings(dataloader):\n",
        "  featureEmbed = []\n",
        "  predictedDigit = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images= images.to(dev)\n",
        "        labels = labels.to(dev)\n",
        "        featureEmbed.append(net(images, feature_embedding=True).to(\"cpu\").numpy())\n",
        "        pred = net(images)\n",
        "        predictedDigit.append(torch.max(pred, 1)[1].data.squeeze().to(\"cpu\").numpy())\n",
        "\n",
        "  # flatten lists\n",
        "  featureEmbed = np.array(list(itertools.chain(*featureEmbed)))\n",
        "  predictedDigit = np.array(list(itertools.chain(*predictedDigit)))\n",
        "\n",
        "  return featureEmbed, predictedDigit"
      ],
      "metadata": {
        "id": "Cay1LKG1dCgt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need the data with the labels\n",
        "trainFeatureEmbed, trainPredictedDigit = getFeatureEmbedings(trainloader)"
      ],
      "metadata": {
        "id": "RXoE9MVsncaB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [6] Code: Match cluster labels to actual Fashion MNIST labels**"
      ],
      "metadata": {
        "id": "cP92A4A4FnBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to see how well k-means did we can use this supervised method of defining what a cluster is by seting the cluster label as the most common digits in that cluster\n",
        "#unsupervised approaches include: manually selecting class depending on mean image\n",
        "def retrieve_cluster_to_classification(cluster_labels,y_train):\n",
        "  reference_labels = {}\n",
        "# For loop to run through each label of cluster label\n",
        "  for i in range(len(np.unique(kmeans.labels_))):\n",
        "    index = np.where(cluster_labels == i,1,0)\n",
        "    # we only read 0:NUM_CLASSES so we dont read the unknown labels\n",
        "    num = np.bincount(y_train[index==1])[:NUM_CLASSES].argmax()\n",
        "    reference_labels[i] = num\n",
        "    # TODO: Right now the refrence label just maps to the majority label, should we also consider the 2nd and 3rd highest\n",
        "  return reference_labels"
      ],
      "metadata": {
        "id": "N4J6Y94UyRci"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) [7] Code: Define retrained CNN function using predicted labels from clustering for unlabeled data**\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ91J2ujyL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrain_CNN_with_predicted_labels(train_dataset, my_labeled_trainset, my_unlabeled_trainset, unlabeled_train_loader, cluster_method):\n",
        "  \n",
        "  # remove all unlabeled labels from main training set, set them to NUM_CLASSES\n",
        "  train_targets = np.array(train_dataset.targets.numpy(), copy=True)  \n",
        "  train_targets[my_unlabeled_trainset.indices] = NUM_CLASSES\n",
        "  \n",
        "  # mapping from NUM_CLUSTERS to FASHION_MNIST classes\n",
        "  reference_labels = retrieve_cluster_to_classification(\n",
        "      cluster_method.labels_, \n",
        "      train_targets\n",
        "  )\n",
        "\n",
        "  unlabelTrainFeatureEmbed, _ = getFeatureEmbedings(unlabeled_train_loader)\n",
        "  \n",
        "  # Assign new labels\n",
        "  predicted_test = cluster_method.predict(unlabelTrainFeatureEmbed)\n",
        "  \n",
        "  for i in range(unlabelTrainFeatureEmbed.shape[0]):\n",
        "    my_unlabeled_trainset.dataset.targets[my_unlabeled_trainset.indices[i]] = reference_labels[predicted_test[i]]\n",
        "\n",
        "  new_combined_data = torch.utils.data.ConcatDataset([my_labeled_trainset, my_unlabeled_trainset])\n",
        "  new_combined_data_loader = DataLoader(new_combined_data, batch_size=128, shuffle=True)\n",
        "  retrained_net = BasicNet()\n",
        "  retrained_net.to(dev)\n",
        "  result, retrained_net = train(NUM_EPOCHS, retrained_net, new_combined_data_loader)\n",
        "  return retrained_net\n",
        "\n"
      ],
      "metadata": {
        "id": "xwS6S-FokaZJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Experiments**\n"
      ],
      "metadata": {
        "id": "KjveyCWCmUnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment A) First we will create three experiments using diffirent ratios of M:N-M\n",
        "\n",
        "\n",
        "1.   Ratio of 70% labeled to 30% unlabeled\n",
        "2.   Ratio of 50% labeled to 50% unlabeled\n",
        "3.   Ratio of 30% labeled to 70% unlabeled\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zk9ebry0n4lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment A: Diffirent Ratios of M to N\n",
        "\n",
        "labeled_train_loader_70, unlabeled_train_loader_30, labeled_train_data_70, unlabeled_train_data_30 = splitTrainingData(trainset, 0.7)\n",
        "labeled_train_loader_50, unlabeled_train_loader_50, labeled_train_data_50, unlabeled_train_data_50 = splitTrainingData(trainset, 0.5)\n",
        "labeled_train_loader_30, unlabeled_train_loader_70, labeled_train_data_30, unlabeled_train_data_70 = splitTrainingData(trainset, 0.3)"
      ],
      "metadata": {
        "id": "njxlymmEn6UR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment B) Then we will try training it with diffirent clustering methods"
      ],
      "metadata": {
        "id": "gU2k8HEDrNYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "kmeans = MiniBatchKMeans(n_clusters = NUM_CLUSTERS)\n",
        "\n",
        "kmeans.fit(trainFeatureEmbed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAss2NNi-6-z",
        "outputId": "a2d93030-cf21-4627-ea5c-33358923cefe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniBatchKMeans(n_clusters=30)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "gmm = KMeans(n_clusters = NUM_CLUSTERS)\n",
        "\n",
        "gmm.fit(trainFeatureEmbed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDYIOUEK-wo9",
        "outputId": "61921326-3e8d-4b97-eaa7-b365a795ce2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=30)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment C) Train all the models combining diffirent experiments"
      ],
      "metadata": {
        "id": "8dpNhodXA5pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_70_label_30_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_70, unlabeled_train_data_30, unlabeled_train_loader_30, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v01rGfhXBFaU",
        "outputId": "d8493023-7305-4fe0-b60f-b37bafb89bb5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 1.4785804748535156\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 1.2729737758636475\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 1.3090766668319702\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 1.3136640787124634\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 1.2323434352874756\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 1.279140591621399\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 1.097141146659851\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 1.091235876083374\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 1.0787136554718018\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 1.0675567388534546\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 1.2230411767959595\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 1.1279586553573608\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 1.0751893520355225\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 1.2589514255523682\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 1.1153379678726196\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 1.1287935972213745\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 1.0029702186584473\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 1.3196955919265747\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 1.1173112392425537\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 1.3424274921417236\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.8745275735855103\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 1.1698884963989258\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.8614926934242249\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.9223056435585022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_50_label_50_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_50, unlabeled_train_data_50, unlabeled_train_loader_50, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz1hF8lNBHR_",
        "outputId": "3f8f398a-c69f-4b83-fefb-439d9f2d2901"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 1.4070624113082886\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 1.1350146532058716\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 1.2456623315811157\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 1.3466253280639648\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 1.0949289798736572\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 1.088902235031128\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 1.0283422470092773\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 1.1369366645812988\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.8658769726753235\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 1.1358987092971802\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 1.1506801843643188\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.9587178826332092\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.9824383854866028\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 1.0999360084533691\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 1.08925199508667\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 1.1529611349105835\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 1.1042420864105225\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 1.1022045612335205\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.9485915899276733\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.8748258352279663\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 1.0657912492752075\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 1.1166856288909912\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.9368566274642944\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.9382832646369934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_kmeans_30_label_70_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_30, unlabeled_train_data_70, unlabeled_train_loader_70, kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjNFDCvOBHam",
        "outputId": "c45a289d-c7fc-4670-b5a7-941dfc2cd38c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 0.5977845788002014\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 0.6786027550697327\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 0.49654868245124817\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 0.5311013460159302\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 0.6410374045372009\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 0.4915721118450165\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.5669322609901428\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 0.5070799589157104\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.39156338572502136\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 0.5083044767379761\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 0.470933198928833\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.36339080333709717\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.44836822152137756\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 0.4707953929901123\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.49060937762260437\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.4809212386608124\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 0.47335031628608704\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 0.4103602170944214\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.5162898302078247\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.3358392119407654\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.38929951190948486\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 0.46386367082595825\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.5088387131690979\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.34224385023117065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_70_label_30_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_70, unlabeled_train_data_30, unlabeled_train_loader_30, gmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-i9wlc7BMDr",
        "outputId": "a1282b99-1f7d-44fe-f2ca-0deadf8b4491"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 0.474614679813385\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 0.4131590723991394\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 0.3221578299999237\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 0.47874459624290466\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 0.30274778604507446\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 0.38786476850509644\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.3099869191646576\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 0.38634538650512695\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.4012346863746643\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 0.3499428927898407\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 0.32617661356925964\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.49075034260749817\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.3801426589488983\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 0.3631058931350708\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.3931160271167755\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.392886221408844\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 0.4301944971084595\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 0.3360289931297302\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.27845054864883423\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.358793169260025\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.316563218832016\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 0.19952866435050964\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.34761470556259155\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.4327757954597473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_50_label_50_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_50, unlabeled_train_data_50, unlabeled_train_loader_50, gmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSar1kepBMJw",
        "outputId": "7bbb66e7-4782-443e-e423-e75a1f742c9f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 0.3827483654022217\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 0.5807194709777832\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 0.3654506206512451\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 0.36789625883102417\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 0.5412696599960327\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 0.5430489778518677\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.4248042106628418\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 0.2830982506275177\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.3062753975391388\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 0.3688565194606781\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 0.30839142203330994\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.4163661301136017\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.3262276351451874\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 0.23639681935310364\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.37633031606674194\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.248992919921875\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 0.2677233815193176\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 0.31718528270721436\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.3831905722618103\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.41384032368659973\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.4163159430027008\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 0.3663702607154846\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.3549787104129791\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.26601308584213257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrained_net_gmm_30_label_70_not = retrain_CNN_with_predicted_labels(trainset, labeled_train_data_30, unlabeled_train_data_70, unlabeled_train_loader_70, gmm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM5MAfTbBMTA",
        "outputId": "29947762-a098-4b55-a294-b1ae448edee3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 469 , Loss 0.3068472445011139\n",
            "Epoch 1 / 6, Step 200/ 469 , Loss 0.584756076335907\n",
            "Epoch 1 / 6, Step 300/ 469 , Loss 0.42139360308647156\n",
            "Epoch 1 / 6, Step 400/ 469 , Loss 0.4365386962890625\n",
            "Epoch 2 / 6, Step 100/ 469 , Loss 0.4774197041988373\n",
            "Epoch 2 / 6, Step 200/ 469 , Loss 0.5145040154457092\n",
            "Epoch 2 / 6, Step 300/ 469 , Loss 0.37255585193634033\n",
            "Epoch 2 / 6, Step 400/ 469 , Loss 0.4935983121395111\n",
            "Epoch 3 / 6, Step 100/ 469 , Loss 0.2720433473587036\n",
            "Epoch 3 / 6, Step 200/ 469 , Loss 0.4063916802406311\n",
            "Epoch 3 / 6, Step 300/ 469 , Loss 0.3594999313354492\n",
            "Epoch 3 / 6, Step 400/ 469 , Loss 0.2587115466594696\n",
            "Epoch 4 / 6, Step 100/ 469 , Loss 0.5196161866188049\n",
            "Epoch 4 / 6, Step 200/ 469 , Loss 0.40496110916137695\n",
            "Epoch 4 / 6, Step 300/ 469 , Loss 0.36468905210494995\n",
            "Epoch 4 / 6, Step 400/ 469 , Loss 0.38384923338890076\n",
            "Epoch 5 / 6, Step 100/ 469 , Loss 0.36963167786598206\n",
            "Epoch 5 / 6, Step 200/ 469 , Loss 0.31178006529808044\n",
            "Epoch 5 / 6, Step 300/ 469 , Loss 0.382847398519516\n",
            "Epoch 5 / 6, Step 400/ 469 , Loss 0.32824262976646423\n",
            "Epoch 6 / 6, Step 100/ 469 , Loss 0.2621377110481262\n",
            "Epoch 6 / 6, Step 200/ 469 , Loss 0.3195037245750427\n",
            "Epoch 6 / 6, Step 300/ 469 , Loss 0.3606064021587372\n",
            "Epoch 6 / 6, Step 400/ 469 , Loss 0.3186618983745575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results**\n"
      ],
      "metadata": {
        "id": "JGBSP5TXBiGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First we will look at the **baseline** results from the CNN trained on M labeled images"
      ],
      "metadata": {
        "id": "fnVfGL5oZU3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testFeatureEmbed, _ = getFeatureEmbedings(testloader)"
      ],
      "metadata": {
        "id": "9IKInqSi6C1S"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_accuracy, loss = test(testloader, net)\n",
        "print(f\"Our accuracy with just the CNN is: {baseline_accuracy} on test set from training on trainset\" )"
      ],
      "metadata": {
        "id": "pH2XADkWZSER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21985528-48e8-4de6-b206-a425120850f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with just the CNN is: 0.8463 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next we will look at all the other methods that use weakly supervised learning, we aim to be better than the baseline in each\n"
      ],
      "metadata": {
        "id": "vJ2rkCOdZUPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_70_label_30_not, loss = test(testloader, retrained_net_kmeans_70_label_30_not)\n",
        "print(f\"Our accuracy with the CNN and Mini-Batch Kmeans clustering with 70% labeled, 30% unlabeled is: {accuracy_kmeans_70_label_30_not} on test set\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yut5RI8zAxZg",
        "outputId": "74d7ff73-3d67-4631-f0d3-4f9ce131ecb1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Mini-Batch Kmeans clustering with 70% labeled, 30% unlabeled is: 0.8666 on test set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_50_label_50_not, loss = test(testloader, retrained_net_kmeans_50_label_50_not)\n",
        "print(f\"Our accuracy with the CNN and Mini-Batch Kmeans clustering with 50% labeled, 50% unlabeled is: {accuracy_kmeans_50_label_50_not} on test set\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0d8xRVIAxlT",
        "outputId": "eed50357-e894-4207-fd94-8f59a77a9d39"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Mini-Batch Kmeans clustering with 50% labeled, 50% unlabeled is: 0.1824 on test set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_kmeans_30_label_70_not, loss = test(testloader, retrained_net_kmeans_30_label_70_not)\n",
        "print(f\"Our accuracy with the CNN and Mini-Batch Kmeans clustering with 30% labeled, 70% unlabeled is: {accuracy_kmeans_30_label_70_not} on test set\" )"
      ],
      "metadata": {
        "id": "IvzHxZpzC9tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b4b88f-b93b-49b6-cc31-a68c1cbcd18b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Mini-Batch Kmeans clustering with 30% labeled, 70% unlabeled is: 0.1001 on test set\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_gmm_70_label_30_not, loss = test(testloader, retrained_net_gmm_70_label_30_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 70% labeled, 30% unlabeled is: {accuracy_gmm_70_label_30_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V2K0kkdCjJ6",
        "outputId": "bbc1e2de-5447-4f62-c5af-d187a8149f4b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 70% labeled, 30% unlabeled is: 0.1002 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_gmm_50_label_50_not, loss = test(testloader, retrained_net_gmm_50_label_50_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 50% labeled, 50% unlabeled is: {accuracy_gmm_50_label_50_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmVV-eFACjUc",
        "outputId": "02f87d34-ce56-49a4-96a3-307270ee38a8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 50% labeled, 50% unlabeled is: 0.1 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_gmm_30_label_70_not, loss = test(testloader, retrained_net_gmm_30_label_70_not)\n",
        "print(f\"Our accuracy with the CNN and Kmeans clustering with 30% labeled, 70% unlabeled is: {accuracy_gmm_30_label_70_not} on test set from training on trainset\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1zSeky8Cjbu",
        "outputId": "dbd0cabc-c1c1-42e9-80c1-a9ae4528da14"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with the CNN and Kmeans clustering with 30% labeled, 70% unlabeled is: 0.1 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusions**\n",
        "\n",
        "One positive result from our experiments is that we found Mini-Batch Kmeans with a ratio of 70% labeled, 30% unlabeled yielded a 2% increase in accuracy compared to the baseline. However we saw that all other ratios and clustering methods provided a < 20% accuracy rate. Mini Batch K-means in general was more accurate than just pure k-means.\n",
        "\n",
        "\n",
        "The usefulness of ratios where unlabeled was equal to or greater that would have been more of a useful result, but an improvement from 30% unlabeled to \n",
        "86.6% accuracy is not a total loss.\n",
        "\n",
        "\n",
        "We were limited in which clustering methods we could use due to RAM capacity and learning times. GMM was used originally but it would take 2 hours to train with a GPU, and would often crash. DBSCAN, and OPTICS models were also attempted but faced similar issues. \n",
        "\n",
        "\n",
        "One reason we believe that the results weren't as strong as we expected is that we randomly split the data into M% labeled and (N-M)% unlabeled, without taking into consideration the labels of the data. This could have lead to hotspots of certain label types being placed in the unlabeled set, leading to poor classification results.\n",
        "\n",
        "\n",
        "Another reason we believe that could explain the result, is that we performed clustering on the feature emebedings of the images. Meaning that after we trained our original CNN on M labeled images, we used it to get a vector of size 4096 from each image in the training set. We performed clustering on the feature embeded version of each image, and 4096 might have been too small of a feature to encode all the data of an image required to cluster it properly (although in prior tests we reached 86% accuracy just from clustering using a full labeled N size dataset).\n",
        "\n",
        "Lastly we believe that the way that we assigned labels to the unlabeled images might be improved. We used the mode label in each cluster (from the labeled images in the cluster). We believe that in the future we should have only labeled images with a probability greater than a certain threshold as the label, as there were 30 clusters, and noisy/inacurate labels could have been introduced."
      ],
      "metadata": {
        "id": "1GxL84V_Rv82"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of cs484 proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}