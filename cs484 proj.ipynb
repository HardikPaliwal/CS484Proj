{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HardikPaliwal/CS484Proj/blob/master/cs484%20proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOiQJ0-YBNR8"
      },
      "source": [
        "# **CS484 Final Project**\n",
        "\n",
        "#### Topic 6: Weakly supervised classification\n",
        "\n",
        "#### Hardik Paliwal (20725413), Lance Pereira (20719626)\n",
        "\n",
        "______________________________________________________________"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Table Of Contents**\n",
        "- A) Abstract\n",
        "- B) High Level Goals and Methedology\n",
        "- C) Team Members and Contributions\n",
        "- D) External Code Libraries\n",
        "- E) Code\n",
        "  - 1) Setup (imports, loading data)\n",
        "  - 2) Our CNN (Based on VGG11)\n",
        "  - 3) Clustering Methods from SKlearn \n",
        "- F) Experiments\n",
        "  - 1) Diffirent ratios of M/N\n",
        "  - 2) Diffirent Clustering Methods\n",
        "  - 3) Clustering as Final vs Retrained CNN\n",
        "- G) Results\n",
        "  - 1) Results of diffirent ratios\n",
        "  - 2) Results from diffirent clustering methods\n",
        "  - 3) Results from Clustering vs retrained CNN\n",
        "- H) Conclusions"
      ],
      "metadata": {
        "id": "P5a9RiUbRKX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDBcYS6VBNSA"
      },
      "source": [
        "#**A) Abstract:**\n",
        "\n",
        "For our project we have decided to do Project 6, choosing specifically Fashion MNIST. We use weakly supervised classification to try and improve results compared to just using supervised learning.\n",
        " \n",
        "\n",
        "#**B) High Level Goals and Methedology:**\n",
        "\n",
        "Out high level goal is to use weakly supervised classification to improve our \n",
        "prediction ability compared to just training using labeled images. We hope to \n",
        "achieve atleast a 5% increase in our test prediction score using clustering \n",
        "methods.\n",
        "\n",
        "Our method is split into two experiments, \n",
        "- we first will test to see training\n",
        "a CNN on N-M labeled images, then use a cluestering method to classify all the \n",
        "N images, using the majority label in each cluster as the predicted label\n",
        "- secondly we will try using the predicted labels from the previous step to \n",
        "train a new CNN model, to see if it performs better\n",
        "\n",
        "Our baseline will be a simple CNN of N+1 (with the K extra classes being labelled as \"unknown\") to differentiate between unlabeled and labeled classes. Then simply run (some unsupervised model) on the unlabeled data. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**C) Team Members and Contributions**\n",
        "\n",
        "- Hardik Paliwal\n",
        "  - Created CNN based on VGG11 for training\n",
        "  - Created function to gather features from pretrained CNN (trained on M labeled images)\n",
        "  - Created method to get predicted labels from clustering methods\n",
        "\n",
        "- Lance Pereira\n",
        "  - (TODO) Modified VGG11 like CNN for better results\n",
        "  - Created experiments for ratios of M/N \n",
        "  - Split data into labeled and unlabeled\n",
        "  - Created experiments for diffirent clustering methods\n",
        "    - Kmeans, Kmedians, Kmodes\n",
        "    - GMM\n",
        "  - Created experiments for final classification using Kmeans vs using predicted labels to retrain CNN"
      ],
      "metadata": {
        "id": "3ZbWhkZiOTHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**D) External Code Libraries**\n",
        "\n",
        "We used\n",
        "\n",
        "- Pytorch\n",
        "  - Because it was crucial for the quick training of our models\n",
        "  - Allowed us to not have to deal with calculating back propogation\n",
        "- Sklearn\n",
        "  - Provided us a large range of clustering methods for quick experimentation\n",
        "- Numpy\n",
        "  - Useful for large matrix operations"
      ],
      "metadata": {
        "id": "YHKO718hQ17r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "phv5gD4DBNSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision as tv\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "import sklearn\n",
        "from torch import optim\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **E) Code**"
      ],
      "metadata": {
        "id": "y_GC_Ou-aqJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Setup**\n",
        "\n",
        "The below cells:\n",
        "- Import Fshion MNIST\n",
        "- define functions that strip label from data"
      ],
      "metadata": {
        "id": "ev3-3ZQobP0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "umKKVD1LBNSE"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "dev=torch.device(\"cuda\") \n",
        "NUM_EPOCHS = 6\n",
        "NUM_CLUSTERS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "lpBIjSHXBNSF"
      },
      "outputs": [],
      "source": [
        "trainset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=True,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "# trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "testset = tv.datasets.FashionMNIST(root=\"./\", download=True,train=False,  transform=tv.transforms.Compose(\n",
        "    [tv.transforms.Resize(32), tv.transforms.ToTensor()]))\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Define our Base CNN (Based on VGG11), Test, Train methods**"
      ],
      "metadata": {
        "id": "o24XBil8bahy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfzjkgNwBNSI"
      },
      "outputs": [],
      "source": [
        "#This is an implementation of VGG11 (which is a precursor to VGG16) for mnist dataset.\n",
        "# it also takes in n, which is the number of classes. N+1 class stands for unknown. \n",
        "\n",
        "#this will let us differeniate the unlabelled data from the labelled data\n",
        "class BasicNet(nn.Module):\n",
        "    def __init__(self, n=9):\n",
        "        super(BasicNet, self).__init__()\n",
        "        self.batchNorm = [nn.BatchNorm2d(64), nn.BatchNorm2d(128),nn.BatchNorm2d(256), nn.BatchNorm2d(256),\n",
        "                          nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512), nn.BatchNorm2d(512)]\n",
        "        self.conv = [\n",
        "        nn.Conv2d(1, 64, 3, 1, 1) ,nn.Conv2d(64, 128, 3, 1, 1), nn.Conv2d(128, 256, 3, 1, 1), nn.Conv2d(256, 256, 3, 1, 1)\n",
        "       ,nn.Conv2d(256, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1), nn.Conv2d(512, 512, 3, 1, 1)\n",
        "        ]\n",
        "        maxPool = nn.MaxPool2d(2, stride=2)\n",
        "        self.conv1 = nn.Sequential(self.conv[0], self.batchNorm[0], nn.ReLU(), maxPool)\n",
        "        self.conv2 = nn.Sequential(self.conv[1], self.batchNorm[1], nn.ReLU(), maxPool)\n",
        "        self.conv3 = nn.Sequential(self.conv[2], self.batchNorm[2], nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(self.conv[3], self.batchNorm[3], nn.ReLU(), maxPool) \n",
        "        self.conv5 = nn.Sequential(self.conv[4], self.batchNorm[4], nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(self.conv[5], self.batchNorm[5], nn.ReLU(), maxPool)\n",
        "        self.conv7 = nn.Sequential(self.conv[6], self.batchNorm[6], nn.ReLU()) \n",
        "        self.conv8 = nn.Sequential(self.conv[7], self.batchNorm[7], nn.ReLU(), maxPool)\n",
        "        self.fc1 = nn.Linear(512, 4096)\n",
        "        self.fc2 = nn.Linear(4096, 4096)\n",
        "        self.fc3 = nn.Linear(4096, n+1)\n",
        "        \n",
        "    def forward(self, x, feature_embedding=False):\n",
        "        dropOut = nn.Dropout(p=0.5)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x)\n",
        "        x = self.conv6(x)\n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = dropOut(F.relu(self.fc1(x)))\n",
        "        x = dropOut(F.relu(self.fc2(x)))\n",
        "        if(feature_embedding):\n",
        "          return x\n",
        "        #Not sure why this works without softmax. probably a reasoning givin in the paper (cause the output can range from anything (not normalized to a 0-1 probability range))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZUIoIjzBNSL"
      },
      "outputs": [],
      "source": [
        "def test(data, net):\n",
        "    net.eval()\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_correct = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for i, (images, labels) in enumerate(data):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            test_pred = net(images)\n",
        "\n",
        "            pred = torch.max(test_pred, 1)[1].data.squeeze()\n",
        "            total_correct+= (pred == labels).sum().item()\n",
        "            loss = loss_func( test_pred, labels)\n",
        "            total_loss+= loss.item()*images.size(0)\n",
        "        # return total_correct/len(data.dataset), total_loss/len(data.dataset)\n",
        "        # Note From Lance: We shouldn't divide the total loss\n",
        "        return total_correct/len(data.dataset), total_loss\n",
        "  \n",
        "def train(num_epochs, net, trainloader):\n",
        "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "    accuracy_through_epochs = []\n",
        "    total_step = len(trainloader)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        for i, (images, labels) in enumerate(trainloader):\n",
        "            images= images.to(dev)\n",
        "            labels = labels.to(dev)\n",
        "            optimizer.zero_grad()           \n",
        "            prediction = net(images)\n",
        "            loss = loss_func( prediction, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if ((i +1) % 100 == 0):\n",
        "                print(f\"Epoch {epoch+1} / {num_epochs}, Step {i+1}/ {total_step} , Loss {loss.item()}\")\n",
        "\n",
        "    return accuracy_through_epochs, net"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Split Fashion MNIST training data into M labeled images, and N-M unlabeled**"
      ],
      "metadata": {
        "id": "NK6JVE0UGfF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Modifies dataset in place to only have values correspounding to the labels in classesToUse\n",
        "def splitTrainingData(training_data, M_percent):\n",
        "  # N is len(training_data)\n",
        "  len_N = len(trainset)\n",
        "\n",
        "  # M is the number of labeled images we want\n",
        "  len_M = int(M_percent*len_N)\n",
        "  \n",
        "  labeled_data, unlabeled_data = torch.utils.data.random_split(trainset, [len_M, len_N - len_M])\n",
        "\n",
        "  # strip the labels from unlabeled_data\n",
        "  labeled_data_loader = DataLoader(labeled_data, batch_size=128, shuffle=True)\n",
        "  unlabeled_data_loader = DataLoader(unlabeled_data, batch_size=128, shuffle=True)\n",
        "\n",
        "  return labeled_data_loader, unlabeled_data_loader\n"
      ],
      "metadata": {
        "id": "HaaxUxtcGppA"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_train_loader, unlabeled_train_loader = splitTrainingData(trainset, 0.7)"
      ],
      "metadata": {
        "id": "GaNdStKMkQun"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Train Base CNN on M labeled training images**"
      ],
      "metadata": {
        "id": "iPnEeZO5chxW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Q4oP9MOBNSN",
        "outputId": "a5d7270d-87e8-4524-c0b9-e70da2c45b64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / 6, Step 100/ 329 , Loss 0.511478066444397\n",
            "Epoch 1 / 6, Step 200/ 329 , Loss 0.4268065094947815\n",
            "Epoch 1 / 6, Step 300/ 329 , Loss 0.3081866502761841\n",
            "Epoch 2 / 6, Step 100/ 329 , Loss 0.3177548348903656\n",
            "Epoch 2 / 6, Step 200/ 329 , Loss 0.25087860226631165\n",
            "Epoch 2 / 6, Step 300/ 329 , Loss 0.3186558187007904\n",
            "Epoch 3 / 6, Step 100/ 329 , Loss 0.20357073843479156\n",
            "Epoch 3 / 6, Step 200/ 329 , Loss 0.2981604039669037\n",
            "Epoch 3 / 6, Step 300/ 329 , Loss 0.2449653446674347\n",
            "Epoch 4 / 6, Step 100/ 329 , Loss 0.194676473736763\n",
            "Epoch 4 / 6, Step 200/ 329 , Loss 0.23580771684646606\n",
            "Epoch 4 / 6, Step 300/ 329 , Loss 0.3226121664047241\n",
            "Epoch 5 / 6, Step 100/ 329 , Loss 0.14157429337501526\n",
            "Epoch 5 / 6, Step 200/ 329 , Loss 0.12187501788139343\n",
            "Epoch 5 / 6, Step 300/ 329 , Loss 0.1241547167301178\n",
            "Epoch 6 / 6, Step 100/ 329 , Loss 0.0540289506316185\n",
            "Epoch 6 / 6, Step 200/ 329 , Loss 0.1819475293159485\n",
            "Epoch 6 / 6, Step 300/ 329 , Loss 0.14424601197242737\n"
          ]
        }
      ],
      "source": [
        "net = BasicNet()\n",
        "net.to(dev)\n",
        "result, trained_net = train(NUM_EPOCHS, net, labeled_train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Use the 'semi' trained CNN to gain important features (feature embedings) of N-M *unlabeled* training images**"
      ],
      "metadata": {
        "id": "JgorThJ9GTSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#do this to store the results in 1 numpy array of 10000 images vs like 20 batches of size 128 images. \n",
        "#get memory error when doing it on a batch of size len(trainloader), so we have to combine the results for trainloader\n",
        "\n",
        "def getFeatureEmbedings(dataloader):\n",
        "  featureEmbed = []\n",
        "  predictedDigit = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "        images= images.to(dev)\n",
        "        labels = labels.to(dev)\n",
        "        featureEmbed.append(net(images, feature_embedding=True).to(\"cpu\").numpy())\n",
        "        pred = net(images)\n",
        "        predictedDigit.append(torch.max(pred, 1)[1].data.squeeze().to(\"cpu\").numpy())\n",
        "\n",
        "  # flatten lists\n",
        "  featureEmbed = np.array(list(itertools.chain(*featureEmbed)))\n",
        "  predictedDigit = np.array(list(itertools.chain(*predictedDigit)))\n",
        "\n",
        "  return featureEmbed, predictedDigit"
      ],
      "metadata": {
        "id": "Cay1LKG1dCgt"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainFeatureEmbed, trainPredictedDigit = getFeatureEmbedings(unlabeled_train_loader)"
      ],
      "metadata": {
        "id": "RXoE9MVsncaB"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Train Cluster methods on train N-M *unlabeled* images**"
      ],
      "metadata": {
        "id": "yuce7pGtG0qa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "PsyJRDl2BNSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec39d900-9c09-445a-e701-ade4060bff4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MiniBatchKMeans(n_clusters=30)"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ],
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "kmeans = MiniBatchKMeans(n_clusters = NUM_CLUSTERS)\n",
        "kmeans.fit(trainFeatureEmbed)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#In order to see how well k-means did we can use this supervised method of defining what a cluster is by seting the cluster label as the most common digits in that cluster\n",
        "#unsupervised approaches include: manually selecting class depending on mean image\n",
        "def retrieve_cluster_to_classification(cluster_labels,y_train):\n",
        "  reference_labels = {}\n",
        "# For loop to run through each label of cluster label\n",
        "  for i in range(len(np.unique(kmeans.labels_))):\n",
        "    index = np.where(cluster_labels == i,1,0)\n",
        "    num = np.bincount(y_train[index==1]).argmax()\n",
        "    reference_labels[i] = num\n",
        "  return reference_labels"
      ],
      "metadata": {
        "id": "N4J6Y94UyRci"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **E) Code: Define validation functions a) cluster final, b) retrained CNN**\n",
        "\n"
      ],
      "metadata": {
        "id": "cJ91J2ujyL0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def \n",
        "reference_labels = retrieve_cluster_to_classification(kmeans.labels_,trainset.targets.numpy())\n",
        "number_labels = np.zeros(len(testset)).astype(int)\n",
        "predicted_test = kmeans.predict(testFeatureEmbed)\n",
        "for i in range(testFeatureEmbed.shape[0]):\n",
        "  number_labels[i] = reference_labels[predicted_test[i]]"
      ],
      "metadata": {
        "id": "xwS6S-FokaZJ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset.targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ExppgM_n9B8",
        "outputId": "2f974909-8d3f-4817-f8ed-d57b2b2f4676"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 0, 0,  ..., 3, 0, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Experiments**\n"
      ],
      "metadata": {
        "id": "KjveyCWCmUnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Results**\n"
      ],
      "metadata": {
        "id": "JGBSP5TXBiGv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###First we will look at the baseline results from the CNN trained on M labeled images"
      ],
      "metadata": {
        "id": "fnVfGL5oZU3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "pH2XADkWZSER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vJ2rkCOdZUPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BUT Note our 30 classes for k-means and the use of the supervised cluster classification. We only get around 0.5 with 10 classes.\n",
        "#also note that I get varying results, from 0.9 to 0.99 when running on 30 clusters.\n",
        "target_test = testset.targets.numpy()\n",
        "accuracy = np.sum(np.where(number_labels == target_test, 1, 0)) / target_test.shape[0]\n",
        "print(f\"Our accuracy with kmeans is: {accuracy} on test set from training on trainset\" )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVMCQKfNkb38",
        "outputId": "97352ad1-1858-4250-855d-94e9d339acc0"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our accuracy with kmeans is: 0.8174 on test set from training on trainset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VG-tWrR9whmi"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conclusions**\n",
        "\n",
        "- We found that the most useful ratio of M/N which was 30% labeled, 70% unlabeled yieled\n",
        "- This compared to 50:50\n",
        "- Compared to 70% labeled, 30% unlabeled"
      ],
      "metadata": {
        "id": "1GxL84V_Rv82"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copy of cs484 proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}